---
---

@inproceedings{wang-etal-2024-understanding,
abbr={EMNLP Workshop},
    title = "Understanding Players as if They Are Talking to the Game in a Customized Language: A Pilot Study",
    author = "Wang, Tianze  and
      Honarijahromi, Maryam  and
      Katsarou, Styliani  and
      Mikheeva, Olga  and
      Panagiotakopoulos, Theodoros  and
      Smirnov, Oleg  and
      Cao, Lele  and
      Asadi, Sahar",
    editor = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Park, Chan Young  and
      Shi, Weijia  and
      Hayati, Shirley Anugrah  and
      Tsvetkov, Yulia  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Kang, Dongyeop  and
      Jurgens, David",
    booktitle = "EMNLP Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.customnlp4u-1.5",
    doi = "10.18653/v1/2024.customnlp4u-1.5",
    pages = "47--52",
    pdf="https://aclanthology.org/2024.customnlp4u-1.5.pdf",
    abstract = "This pilot study explores the application of language models (LMs) to model game event sequences, treating them as a customized natural language. We investigate a popular mobile game, transforming raw event data into textual sequences and pretraining a Longformer model on this data. Our approach captures the rich and nuanced interactions within game sessions, effectively identifying meaningful player segments. The results demonstrate the potential of self-supervised LMs in enhancing game design and personalization without relying on ground-truth labels.",
}

@inproceedings{cao-etal-2024-introducing,
abbr={NAACL Workshop},
    title = "Introducing {G}en{C}eption for Multimodal {LLM} Benchmarking: You May Bypass Annotations",
    author = "Cao, Lele  and
      Buchner, Valentin  and
      Senane, Zineb  and
      Yang, Fangkai",
    editor = "Ovalle, Anaelia  and
      Chang, Kai-Wei  and
      Cao, Yang Trista  and
      Mehrabi, Ninareh  and
      Zhao, Jieyu  and
      Galstyan, Aram  and
      Dhamala, Jwala  and
      Kumar, Anoop  and
      Gupta, Rahul",
    booktitle = "NAACL Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.trustnlp-1.16",
    doi = "10.18653/v1/2024.trustnlp-1.16",
    pages = "196--201",
    pdf="https://aclanthology.org/2024.trustnlp-1.16.pdf",
    code="https://github.com/llcresearch/GenCeption",
    supp="https://aclanthology.org/attachments/2024.trustnlp-1.16.SupplementaryMaterial.zip",
    google_scholar_id={YFjsv_pBGBYC},
    abstract = "Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models{'} inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption{'}s efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.",
}

@InProceedings{10.1007/978-3-031-72356-8_25,
abbr={ICANN},
author="Cao*, Lele
and Halvardsson*, Gustaf
and McCornack, Andrew
and von Ehrenheim, Vilhelm
and Herman, Pawel",
editor="Wand, Michael
and Malinovsk{\'a}, Krist{\'i}na
and Schmidhuber, J{\"u}rgen
and Tetko, Igor V.",
title="Beyond Gut Feel: Using Time Series Transformers to Find Investment Gems",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="373--388",
abstract="This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data processing. Our extensive experiments on two real-world investment tasks, benchmarked towards three popular baselines, demonstrate the effectiveness of our approach in improving decision making within the VC and GC industry.",
isbn="978-3-031-72356-8",
doi="10.1007/978-3-031-72356-8_25",
arxiv="https://arxiv.org/abs/2309.16888",
blog="https://motherbrain.ai/applying-transformers-to-score-potentially-successful-startups-7893284efb01"
}


@inproceedings{10.1145/3637528.3671673,
abbr={SIGKDD},
author = {Senane*, Zineb and Cao*, Lele and Buchner, Valentin Leonhard and Tashiro, Yusuke and You, Lei and Herman, Pawel Andrzej and Nordahl, Mats and Tu, Ruibo and von Ehrenheim, Vilhelm},
title = {Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671673},
doi = {10.1145/3637528.3671673},
abstract = {Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2560–2571},
numpages = {12},
keywords = {anomaly detection, classification, clustering, diffusion model, forecasting, imputation, interpolation, multivariate time series, representation learning, self-supervised learning, time series modeling},
location = {Barcelona, Spain},
series = {KDD '24},
selected={true},
google_scholar_id={ns9cj8rnVeAC},
dimensions={true},
pdf={https://dl.acm.org/doi/pdf/10.1145/3637528.3671673},
video={https://youtu.be/pXatG4Tf0r4?si=-S8IFX0EkqptszfR},
code={https://github.com/llcresearch/TSDE},
blog={https://motherbrain.ai/advanced-financial-forecasting-for-portfolio-performance-61a93e55d46b}
}

@inproceedings{10.1145/3637528.3671515,
abbr={SIGKDD},
author = {Cao, Lele and von Ehrenheim, Vilhelm and Granroth-Wilding, Mark and Anselmo Stahl, Richard and McCornack, Andrew and Catovic, Armin and Cavalcanti Rocha, Dhiana Deva},
title = {CompanyKG2: A Large-Scale Heterogeneous Graph for Company Similarity Quantification},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {10.1145/3637528.3671515},
doi = {10.1145/3637528.3671515},
abstract = {This paper presents CompanyKG (version 2), a large-scale heterogeneous graph developed for fine-grained company similarity quantification and relationship prediction, crucial for applications in the investment industry such as market mapping, competitor analysis, and mergers and acquisitions. CompanyKG comprises 1.17 million companies represented as graph nodes, enriched with company description embeddings, and 51.06 million weighted edges denoting 15 distinct inter-company relations. To facilitate a thorough evaluation of methods for company similarity quantification and relationship prediction, we have created four annotated evaluation tasks: similarity prediction, competitor retrieval, similarity ranking, and edge prediction. We offer extensive benchmarking results for 11 reproducible predictive methods, categorized into three groups: node-only, edge-only, and node+edge. To our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset derived from a real-world investment platform, specifically tailored for quantifying inter-company similarity and relationships.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4816–4827},
numpages = {12},
keywords = {benchmark, company similarity quantification, edge prediction, graph neural network, investment, knowledge graph, private equity},
location = {Barcelona, Spain},
series = {KDD '24},
google_scholar_id={O3NaXMp0MMsC},
pdf={https://arxiv.org/pdf/2306.10649},
video={https://youtu.be/yw4fTDXrJhI?si=e4cDGkFLjsoXvmA8},
code={https://github.com/llcresearch/CompanyKG2},
website={https://zenodo.org/records/11391315}
}

@InProceedings{Wang_2022_CVPR,
abbr={CVPR},
    author    = {Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe},
    title     = {Multimodal Token Fusion for Vision Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {12186-12195},
    doi={10.1109/CVPR52688.2022.01187},
    dimensions={true},
    google_scholar_id={IWHjjKOFINEC},
    selected={true},
    abstract={Many adaptations of transformers have emerged to address the single-modal vision tasks, where self-attention modules are stacked to handle input sources like images. Intuitively, feeding multiple modalities of data to vision transformers could improve the performance, yet the inner-modal attentive weights may be diluted, which could thus greatly undermine the final performance. In this paper, we propose a multimodal token fusion method (TokenFusion), tailored for transformer-based vision tasks. To effectively fuse multiple modalities, TokenFusion dynamically detects uninformative tokens and substitute these tokens with projected and aggregated inter-modal features. Residual positional alignment is also adopted to enable explicit utilization of the inter-modal alignments after fusion. The design of TokenFusion allows the transformer to learn correlations among multimodal features, while the single-modal transformer architecture remains largely intact. Extensive experiments are conducted on a variety of homogeneous and heterogeneous modalities and demonstrate that TokenFusion surpasses state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection with point cloud and images.},
    pdf={https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf},
code={https://github.com/yikaiw/TokenFusion},
video={https://youtu.be/aQVBuAjoOXU?si=GB2OhuBAmaVsgC9K},
supp={https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Multimodal_Token_Fusion_CVPR_2022_supplemental.pdf}
}

@inproceedings{ijcai2023p38,
abbr={IJCAI},
  title     = {Measuring Acoustics with Collaborative Multiple Agents},
  author    = {Yu, Yinfeng and Chen, Changan and Cao, Lele and Yang, Fangkai and Sun, Fuchun},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {335--343},
  year      = {2023},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2023/38},
  url       = {https://doi.org/10.24963/ijcai.2023/38},
  pdf={https://www.ijcai.org/proceedings/2023/0038.pdf},
  code={https://github.com/yyf17/MACMA},
  slides={https://yyf17.github.io/MACMA/files/IJCAI2023_MACMA.pdf},
  arxiv={2310.05368},
  website={https://yyf17.github.io/MACMA},
  abstract={As humans, we hear sound every second of our life. The sound we hear is often affected by the acoustics of the environment surrounding us. For example, a spacious hall leads to more reverberation. Room Impulse Responses (RIR) are commonly used to characterize environment acoustics as a function of the scene geometry, materials, and source/receiver locations. Traditionally, RIRs are measured by setting up a loudspeaker and microphone in the environment for all source/receiver locations, which is time-consuming and inefficient. We propose to let two robots measure the environment's acoustics by actively moving and emitting/receiving sweep signals. We also devise a collaborative multi-agent policy where these two robots are trained to explore the environment's acoustics while being rewarded for wide exploration and accurate prediction. We show that the robots learn to collaborate and move to explore environment acoustics while minimizing the prediction error. To the best of our knowledge, we present the very first problem formulation and solution to the task of collaborative environment acoustics measurements with multiple agents.}
}


@inproceedings{cao-etal-2021-pause,
abbr={EMNLP},
    title = "{PAUSE}: Positive and Annealed Unlabeled Sentence Embedding",
    author = "Cao, Lele  and
      Larsson, Emil  and
      von Ehrenheim, Vilhelm  and
      Cavalcanti Rocha, Dhiana Deva  and
      Martin, Anna  and
      Horn, Sonja",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    dimensions={true},
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.791",
    doi = "10.18653/v1/2021.emnlp-main.791",
    pages = "10096--10107",
    video = "https://aclanthology.org/2021.emnlp-main.791.mp4",
    pdf="https://aclanthology.org/2021.emnlp-main.791.pdf",
    code = "https://github.com/llcresearch/pause",
    abstract = "Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared to the unsupervised methods, the supervised ones make less assumptions about optimization objectives and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach {--} PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our dataset without the burden of extensive manual annotation work.",
}

@article{10.1162/neco_a_01579,
abbr={Journal},
    author = {Yu*, Yinfeng and Cao*, Lele and Sun, Fuchun and Yang, Chao and Lai, Huicheng and Huang, Wenbing},
    title = {Echo-Enhanced Embodied Visual Navigation},
    journal = {Neural Computation},
    volume = {35},
    number = {5},
    pages = {958-976},
    year = {2023},
    month = {04},
    abstract = {Visual navigation involves a movable robotic agent striving to reach a point goal (target location) using vision sensory input. While navigation with ideal visibility has seen plenty of success, it becomes challenging in suboptimal visual conditions like poor illumination, where traditional approaches suffer from severe performance degradation. We propose E3VN (echo-enhanced embodied visual navigation) to effectively perceive the surroundings even under poor visibility to mitigate this problem. This is made possible by adopting an echoer that actively perceives the environment via auditory signals. E3VN models the robot agent as playing a cooperative Markov game with that echoer. The action policies of robot and echoer are jointly optimized to maximize the reward in a two-stream actor-critic architecture. During optimization, the reward is also adaptively decomposed into the robot and echoer parts. Our experiments and ablation studies show that E3VN is consistently effective and robust in point goal navigation tasks, especially under nonideal visibility.},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01579},
    dimensions={true},
    google_scholar_id={RHpTSmoSYBkC},
    url = {https://doi.org/10.1162/neco\_a\_01579},
    pdf={https://yyf17.github.io/E3VN/files/E3VN.pdf},
    website={https://yyf17.github.io/E3VN/},
    eprint = {https://direct.mit.edu/neco/article-pdf/35/5/958/2079357/neco\_a\_01579.pdf},
}

@InProceedings{10.1007/978-3-030-67658-2_7,
abbr={ECML-PKDD},
author="Cao, Lele
and Asadi, Sahar
and Zhu, Wenfei
and Schmidli, Christian
and Sj{\"o}berg, Michael",
editor="Hutter, Frank
and Kersting, Kristian
and Lijffijt, Jefrey
and Valera, Isabel",
title="Simple, Scalable, and Stable Variational Deep Clustering",
doi="10.1007/978-3-030-67658-2_7",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="108--124",
dimensions={true},
google_scholar_id={Wp0gIr-vW9MC},
abstract="Deep clustering (DC) has become the state-of-the-art for unsupervised clustering. In principle, DC represents a variety of unsupervised methods that jointly learn the underlying clusters and the latent representation directly from unstructured datasets. However, DC methods are generally poorly applied due to high operational costs, low scalability, and unstable results. In this paper, we first evaluate several popular DC variants in the context of industrial applicability using eight empirical criteria. We then choose to focus on variational deep clustering (VDC) methods, since they mostly meet those criteria except for simplicity, scalability, and stability. To address these three unmet criteria, we introduce four generic algorithmic improvements: initial {\$}{\$}{\backslash}gamma {\$}{\$}$\gamma$-training, periodic {\$}{\$}{\backslash}beta {\$}{\$}$\beta$-annealing, mini-batch GMM (Gaussian mixture model) initialization, and inverse min-max transform. We also propose a novel clustering algorithm S3VDC (simple, scalable, and stable VDC) that incorporates all those improvements. Our experiments show that S3VDC outperforms the state-of-the-art on both benchmark tasks and a large unstructured industrial dataset without any ground truth label. In addition, we analytically evaluate the usability and interpretability of S3VDC.",
isbn="978-3-030-67658-2",
pdf="https://arxiv.org/pdf/2005.08047",
code="https://github.com/king/s3vdc",
video="https://slideslive.com/38932370/simple-scalable-and-stable-variational-deep-clustering"
}

@ARTICLE{8031062,
abbr={Journal},
  author={Cao, Lele and Sun, Fuchun and Kotagiri, Ramamohanarao and Huang, Wenbing and Cheng, Weihao and Liu, Xiaolong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Real-Time Recurrent Tactile Recognition: Momentum Batch-Sequential Echo State Networks}, 
  year={2020},
  volume={50},
  number={4},
  pages={1350-1361},
  dimensions={true},
  google_scholar_id={_kc_bZDykSQC},
  keywords={Real-time systems;Reservoirs;Feature extraction;Training;Computational modeling;Tactile sensors;Artificial intelligence;neural networks (NNs);real time systems;tactile sensors;target identification},
  doi={10.1109/TSMC.2017.2746565},
  abstract={Tactile recognition aims at identifying target objects according to tactile sensory readings. Tactile data have two salient properties: 1) sequentially real-time and 2) temporally correlated, which essentially calls for a real-time (i.e., online fixed-budget) and recurrent recognition procedure. Based on an efficient and robust spatio-temporal feature representation for tactile sequences, we handle the problem of real-time recurrent tactile recognition by proposing a bounded online-sequential learning framework, and incorporates the strength of batch-regularization bootstrapping, bounded recursive reservoir, and momentum-based estimation. Experimental evaluations show that it outperforms the state-of-the-art methods by a large margin on test accuracy; and its training performance is superior to most compare},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/319637924_Real-Time_Recurrent_Tactile_Recognition_Momentum_Batch-Sequential_Echo_State_Networks/links/6082c47b8ea909241e1b33db/Real-Time-Recurrent-Tactile-Recognition-Momentum-Batch-Sequential-Echo-State-Networks.pdf}
}

@inproceedings{cao2020debiasing,
abbr={RecSys Workshop},
  title={Debiasing Few-Shot Recommendation in Mobile Games},
  author={Cao, Lele and Asadi, Sahar and Biasielli, Matteo and Sj{\"o}berg, Michael},
  booktitle = "RecSys Workshop on Online Recommender Systems and User Modeling (ORSUM)",
  year={2020},
  pdf={https://ceur-ws.org/Vol-2715/paper4.pdf},
  abstract={Mobile gaming has become increasingly popular due to the growing usage of smartphones in day to day life. In recent years, this advancement has led to an interest in the application of in-game recommendation systems. However, the in-game recommendation is more challenging than common recommendation scenarios, such as e-commerce, for a number of reasons:(1) the player behavior and context change at a fast pace,(2) only a few items (few-shot) can be exposed, and (3) with an existing hand-crafted heuristic recommendation, performing randomized explorations to collect data is not a business choice that is preferred by game stakeholders. To that end, we propose an end-to-end model called DFSNet (Debiasing Few-Shot Network) that enables training an in-game recommender on an imbalanced dataset that is biased by the existing heuristic policy. We experimentally evaluate the performance of DFSNet both in an offline setup on a validation dataset and online in a real-time serving environment, illustrating the correctness and effectiveness of the trained model.},
  google_scholar_id={9ZlFYXVOiuMC},
  website={https://orsum.inesctec.pt/orsum2020/accepted-papers.php}
}

@inproceedings{loukas-etal-2023-using,
abbr={IJCAI Workshop},
    title = "Using Deep Learning to Find the Next Unicorn: A Practical Synthesis on Optimization Target, Feature Selection, Data Split and Evaluation Strategy",
    author = "Cao, Lele  and
      von Ehrenheim, Vilhelm  and
      Stan, Sebastian  and
      Li, Xiaoxue  and
      Lutz, Alexandra",
    editor = "Chen, Chung-Chi  and
      Takamura, Hiroya  and
      Mathur, Puneet  and
      Sawhney, Remit  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    booktitle = "IJCAI Workshop of Multimodal AI For Financial Forecasting (MuFFin)",
    month = "aug",
    year = "2023",
    address = "Macao",
    publisher = "-",
    pages = "63--73",
    website="https://aclanthology.org/2023.finnlp-1.6/",
    pdf="https://aclanthology.org/2023.finnlp-1.6.pdf",
    abstract="Startups represent newly established business models associated with disruptive innovation and high scalability, hence strongly propel the economic and social development. Meanwhile, startups are heavily constrained by many factors such as limited financial funding and human resources. Therefore, the chance for a startup to succeed is rare like “finding a unicorn in the wild”. Venture Capital strives to identify and invest in unicorn startups as early as possible, hoping to gain a high return. This work is traditionally manual and empirical, making it inherently biased and hard to scale. Recently, the rapid growth of data volume and variety is quickly ushering in deep learning (DL) as a potentially superior approach in this domain. In this work, we carry out a literature review and synthesis on DL-based approaches, emphasizing four key aspects: optimization target, feature selection, data split, and evaluation strategy. For each aspect, we summarize our in-depth understanding and practical learning.",
    google_scholar_id="TQgYirikUcIC",
    arxiv="2210.14195v2",
    selected="true",
    slides="https://www.slideshare.net/slideshow/using-deep-learning-to-find-the-next-unicorn-a-practical-synthesis-on-optimization-target-feature-selection-data-split-and-evaluation-strategy/269599584",
    blog="https://motherbrain.ai/using-deep-learning-to-find-the-next-unicorn-a-practical-synthesis-272dc7e85cb5"
}

@inproceedings{cao-etal-2023-scalable,
abbr={IJCAI Workshop},
    title = "A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models",
    author = "Cao, Lele  and
      von Ehrenheim, Vilhelm  and
      Berghult, Astrid  and
      Henje, Cecilia  and
      Stahl, Richard Anselmo  and
      Wandborg, Joar  and
      Stan, Sebastian  and
      Catovic, Armin  and
      Ferm, Erik  and
      Ingelhag, Hannes",
    editor = "Chen, Chung-Chi  and
      Takamura, Hiroya  and
      Mathur, Puneet  and
      Sawhney, Remit  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    booktitle = "IJCAI Workshop of Financial Technology and Natural Language Processing (FinNLP)",
    month = "Aug",
    year = "2023",
    address = "Macao",
    publisher = "-",
    website = "https://aclanthology.org/2023.finnlp-1.5/",
    pages = "55--62",
    abstract="The Private Equity (PE) firms operate investment funds by acquiring and managing companies to achieve a high return upon selling. Many PE funds are thematic, meaning investment professionals aim to identify trends by covering as many industry sectors as possible, and picking promising companies within these sectors. So, inferring sectors for companies is critical to the success of thematic PE funds. In this work, we standardize the sector framework and discuss the typical challenges; we then introduce our sector inference system addressing these challenges. Specifically, our system is built on a medium-sized generative language model, finetuned with a prompt + model tuning procedure. The deployed model demonstrates a superior performance than the common baselines. The system has been serving many PE professionals for over a year, showing great scalability to data volume and adaptability to any change in sector framework and/or annotation.",
    pdf={https://aclanthology.org/2023.finnlp-1.5.pdf},
    arxiv={https://arxiv.org/abs/2306.03313},
    google_scholar_id="GnPB-g6toBAC"
}

@InProceedings{10.1007/978-3-319-70139-4_59,
abbr={ICONIP},
author="Cao, Lele
and Sun, Fuchun
and Liu, Xiaolong
and Huang, Wenbing
and Cheng, Weihao
and Kotagiri, Ramamohanarao",
editor="Liu, Derong
and Xie, Shengli
and Li, Yuanqing
and Zhao, Dongbin
and El-Alfy, El-Sayed M.",
title="Fix-Budget and Recurrent Data Mining for Online Haptic Perception",
booktitle="Neural Information Processing",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="581--591",
abstract="Haptic perception is to identify different targets from haptic input. Haptic data have two prominent features: sequentially real-time and temporally correlated, which calls for a fixed-budget and recurrent perception procedure. Based on an efficient-robust spatio-temporal feature representation, we handle the problem with a bounded online-sequential learning framework (MBS-ESN), and incorporates the strength of batch-regularization bootstrapping, bounded recursive reservoir, and momentum-based estimation. Experimental evaluations show that it outperforms the state-of-the-art methods by a large margin on test accuracy; and its training performance is superior to most compared models from aspects of computational complexity and storage efficiency.",
isbn="978-3-319-70139-4",
doi={10.1007/978-3-319-70139-4_59}
}

@inproceedings{cao2016efficient,
abbr={AAAI},
  title={Efficient spatio-temporal tactile object recognition with randomized tiling convolutional networks in a hierarchical fusion strategy},
  author={Cao, Lele and Kotagiri, Ramamohanarao and Sun, Fuchun and Li, Hongbo and Huang, Wenbing and Aye, Zay Maung Maung},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016},
  dimensions={true},
  selected={true},
  google_scholar_id={MXK_kJrjxJIC},
  doi={10.1609/aaai.v30i1.10412},
  abstract={Robotic tactile recognition aims at identifying target objects or environments from tactile sensory readings. The advancement of unsupervised feature learning and biological tactile sensing inspire us proposing the model of 3T-RTCN that performs spatio-temporal feature representation and fusion for tactile recognition. It decomposes tactile data into spatial and temporal threads, and incorporates the strength of randomized tiling convolutional networks. Experimental evaluations show that it outperforms some state-of-the-art methods with a large margin regarding recognition accuracy, robustness, and fault-tolerance; we also achieve an order-of-magnitude speedup over equivalent networks with pretraining and finetuning. Practical suggestions and hints are summarized in the end for effectively handling the tactile data.},
  pdf={https://ojs.aaai.org/index.php/AAAI/article/view/10412/10271}
}

@INPROCEEDINGS{8490442,
abbr={CoG},
  author={Gudmundsson, Stefan Freyr and Eisen, Philipp and Poromaa, Erik and Nodet, Alex and Purmonen, Sami and Kozakowski, Bartlomiej and Meurling, Richard and Cao, Lele},
  booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG, n.k.a. CoG)}, 
  title={Human-Like Playtesting with Deep Learning}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  dimensions={true},
  selected={true},
  google_scholar_id={qxL8FJ1GzNcC},
  keywords={Games;Measurement;Training;Machine learning;Supervised learning;Convolutional neural networks;Monte Carlo methods;deep learning;convolutional neural network;agent simulation;playtesting;Monte-Carlo tree search},
  doi={10.1109/CIG.2018.8490442},
  pdf={https://www.researchgate.net/profile/Stefan-Gudmundsson-2/publication/328307928_Human-Like_Playtesting_with_Deep_Learning/links/5bcf1cd992851c1816baf8d1/Human-Like-Playtesting-with-Deep-Learning.pdf},
  abstract={We present an approach to learn and deploy human-like playtesting in computer games based on deep learning from player data. We are able to learn and predict the most "human" action in a given position through supervised learning on a convolutional neural network. Furthermore, we show how we can use the learned network to predict key metrics of new content - most notably the difficulty of levels. Our player data and empirical data come from Candy Crush Saga (CCS) and Candy Crush Soda Saga (CCSS). However, the method is general and well suited for many games, in particular where content creation is sequential. CCS and CCSS are non-deterministic match-3 puzzle games with multiple game modes spread over a few thousand levels, providing a diverse testbed for this technique. Compared to Monte Carlo Tree Search (MCTS) we show that this approach increases correlation with average level difficulty, giving more accurate predictions as well as requiring only a fraction of the computation time.},
}

@inproceedings{10.1145/3511808.3557110,
abbr={CIKM},
author = {Cao, Lele and Horn, Sonja and von Ehrenheim, Vilhelm and Anselmo Stahl, Richard and Landgren, Henrik},
title = {Simulation-Informed Revenue Extrapolation with Confidence Estimate for Scaleup Companies Using Scarce Time-Series Data},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557110},
doi = {10.1145/3511808.3557110},
abstract = {Investment professionals rely on extrapolating company revenue into the future (i.e. revenue forecast) to approximate the valuation of scaleups (private companies in a high-growth stage) and inform their investment decision. This task is manual and empirical, leaving the forecast quality heavily dependent on the investment professionals' experiences and insights. Furthermore, financial data on scaleups is typically proprietary, costly and scarce, ruling out the wide adoption of data-driven approaches. To this end, we propose a simulation-informed revenue extrapolation (SiRE) algorithm that generates fine-grained long-term revenue predictions on small datasets and short time-series. SiRE models the revenue dynamics as a linear dynamical system (LDS), which is solved using the EM algorithm. The main innovation lies in how the noisy revenue measurements are obtained during training and inferencing. SiRE works for scaleups that operate in various sectors and provides confidence estimates. The quantitative experiments on two practical tasks show that SiRE significantly surpasses the baseline methods by a large margin. We also observe high performance when SiRE extrapolates long-term predictions from short time-series. The performance-efficiency balance and result explainability of SiRE are also validated empirically. Evaluated from the perspective of investment professionals, SiRE can precisely locate the scaleups that have a great potential return in 2 to 5 years. Furthermore, our qualitative inspection illustrates some advantageous attributes of the SiRE revenue forecasts.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {2954–2963},
numpages = {10},
keywords = {time series extrapolation, simulation, scaleup, revenue forecast, private capital, measurement, linear dynamical system, investment, growth company, financial data, expectation maximization, confidence estimation, company valuation, accounting, Kalman filter},
location = {Atlanta, GA, USA},
series = {CIKM '22},
pdf={https://arxiv.org/pdf/2208.10375},
code={https://github.com/llcresearch/sire},
video={https://storage.googleapis.com/sire-appendix/CIKM22-app140.mp4},
poster={https://storage.googleapis.com/sire-appendix/app140-poster-36x48.pdf},
slides={https://storage.googleapis.com/sire-appendix/app140-slides.pdf},
blog={https://motherbrain.ai/predicting-revenue-for-scaleup-companies-5b07ec7a38cf},
dimensions={true},
google_scholar_id={hFOr9nPyWt4C}
}

@inproceedings{buchner-etal-2024-prompt,
abbr={NAACL},
    title = "Prompt Tuned Embedding Classification for Industry Sector Allocation",
    author = "Buchner*, Valentin  and
      Cao*, Lele  and
      Kalo, Jan-Christoph  and
      Von Ehrenheim, Vilhelm",
    editor = "Yang, Yi  and
      Davani, Aida  and
      Sil, Avi  and
      Kumar, Anoop",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-industry.10",
    doi = "10.18653/v1/2024.naacl-industry.10",
    code="https://github.com/valbuc/PTEC",
    pages = "108--118",
    google_scholar_id="NMxIlDl6LWMC",
    pdf="https://aclanthology.org/2024.naacl-industry.10.pdf",
    video="https://storage.googleapis.com/motherbrain-ptec/PTEC%20NAACL.mp4",
    slides="https://storage.googleapis.com/motherbrain-ptec/PTEC%20NAACL_slides.pdf",
    poster="https://storage.googleapis.com/motherbrain-ptec/PTEC%20NAACL%20poster.pdf",
    abstract = "We introduce Prompt Tuned Embedding Classification (PTEC) for classifying companies within an investment firm{'}s proprietary industry taxonomy, supporting their thematic investment strategy. PTEC assigns companies to the sectors they primarily operate in, conceptualizing this process as a multi-label text classification task. Prompt Tuning, usually deployed as a text-to-text (T2T) classification approach, ensures low computational cost while maintaining high task performance. However, T2T classification has limitations on multi-label tasks due to the generation of non-existing labels, permutation invariance of the label sequence, and a lack of confidence scores. PTEC addresses these limitations by utilizing a classification head in place of the Large Language Models (LLMs) language head. PTEC surpasses both baselines and human performance while lowering computational demands. This indicates the continuing need to adapt state-of-the-art methods to domain-specific tasks, even in the era of LLMs with strong generalization abilities.",
}

@InProceedings{Wang_2022_CVPR,
abbr={CVPR},
    author    = {Wang, Yikai and Ye, TengQi and Cao, Lele and Huang, Wenbing and Sun, Fuchun and He, Fengxiang and Tao, Dacheng},
    title     = {Bridged Transformer for Vision and Point Cloud 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {12114-12123},
    abstract={3D object detection is a crucial research topic in computer vision, which usually uses 3D point clouds as input in conventional setups. Recently, there is a trend of leveraging multiple sources of input data, such as complementing the 3D point cloud with 2D images that often have richer color and fewer noises. However, due to the heterogeneous geometrics of the 2D and 3D representations, it prevents us from applying off-the-shelf neural networks to achieve multimodal fusion. To that end, we propose Bridged Transformer (BrT), an end-to-end architecture for 3D object detection. BrT is simple and effective, which learns to identify 3D and 2D object bounding boxes from both points and image patches. A key element of BrT lies in the utilization of object queries for bridging 3D and 2D spaces, which unifies different sources of data representations in Transformer. We adopt a form of feature aggregation realized by point-to-patch projections which further strengthen the interaction between images and points. Moreover, BrT works seamlessly for fusing the point cloud with multi-view images. We experimentally show that BrT surpasses state-of-the-art methods on SUN RGB-D and ScanNetV2 datasets.},
    pdf={https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Bridged_Transformer_for_Vision_and_Point_Cloud_3D_Object_Detection_CVPR_2022_paper.pdf},
    supp={https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Bridged_Transformer_for_CVPR_2022_supplemental.pdf},
    doi={10.1109/CVPR52688.2022.01180},
    dimensions={true},
    google_scholar_id={ZeXyd9-uunAC}
}

@article{CAO201660,
abbr={Journal},
title = {Building feature space of extreme learning machine with sparse denoising stacked-autoencoder},
journal = {Neurocomputing},
volume = {174},
pages = {60-71},
year = {2016},
issn = {0925-2312},
doi = {10.1016/j.neucom.2015.02.096},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215011674},
author = {Le-le Cao and Wen-bing Huang and Fu-chun Sun},
dimensions={true},
google_scholar_id={YsMSGLbcyi4C},
pdf={https://drive.google.com/file/d/17aDb48MPAP2A1DegM6HJgs0oZOa7yyLW/view},
keywords = {Extreme learning machine (ELM), Ridge regression, Feature space, Stacked autoencoder (SAE), Classification, Regression},
abstract = {The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, non-linear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DBN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder – ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications.}
}

@article{Cao2018,
abbr={Journal},
  author    = {Cao, Lele and Sun, Fuchun and Liu, Xiaolong and Huang, Wenbing and Kotagiri, Ramamohanarao and Li, Hongbo},
  title     = {End-to-End ConvNet for Tactile Recognition Using Residual Orthogonal Tiling and Pyramid Convolution Ensemble},
  journal   = {Cognitive Computation},
  volume    = {10},
  number    = {5},
  pages     = {718--736},
  year      = {2018},
  doi       = {10.1007/s12559-018-9568-7},
  dimensions={true},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/325625213_End-to-End_ConvNet_for_Tactile_Recognition_Using_Residual_Orthogonal_Tiling_and_Pyramid_Convolution_Ensemble/links/5ce6f705458515712ebda665/End-to-End-ConvNet-for-Tactile-Recognition-Using-Residual-Orthogonal-Tiling-and-Pyramid-Convolution-Ensemble.pdf},
  google_scholar_id={M3ejUd6NZC8C},
  abstract  = {Tactile recognition enables robots identify target objects or environments from tactile sensory readings. The recent advancement of deep learning and biological tactile sensing inspire us proposing an end-to-end architecture ROTConvPCE-mv that performs tactile recognition using residual orthogonal tiling and pyramid convolution ensemble. Our approach uses stacks of raw frames and tactile flow as dual input, and incorporates the strength of multi-layer OTConvs (orthogonal tiling convolutions) organized in a residual learning paradigm. We empirically demonstrate that OTConvs have adjustable invariance capability to different input transformations such as translation, rotation, and scaling. To effectively capture multi-scale global context, a pyramid convolution structure is attached to the concatenated output of two residual OTConv pathways. The extensive experimental evaluations show that ROTConvPCE-mv outperforms several state-of-the-art methods with a large margin regarding recognition accuracy, robustness, and fault-tolerance. Practical suggestions and hints are summarized throughout this paper to facilitate the effective recognition using tactile sensory data.},
}

@INPROCEEDINGS{8317749,
abbr={ITSC},
  author={Liu, Xiaolong and Deng, Zhidong and Lu, Hongchao and Cao, Lele},
  booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Benchmark for road marking detection: Dataset specification and performance baseline}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Roads;Clouds;Benchmark testing;Automobiles;Cameras;Autonomous automobiles;Meteorology;benchmark;road markings;scene parsing;CNN},
  doi={10.1109/ITSC.2017.8317749},
  dimensions={true},
  google_scholar_id={BqipwSGYUEgC},
  abstract={Detection of drivable road area and other critical objects like obstacles and landmarks in traffic scenes is fundamental to advanced driver assistance systems (ADAS) and self-driving car. Although scene parsing is able to make segmentation of road area from other objects and background, it basically does not get involved in recognizing other on-road markings. In fact, detection and classification of road area are only one small step towards true autonomous driving, because there are many categories of informative markings embodied within road area, such as lane markings, arrows, guiding lines, pedestrian crosswalks, and no-vehicle signs. If system identifies those markings, more information for both ADAS and self-driving car system can be provided. For this purpose, we release a benchmark dataset named TRoM (Tsinghua Road Marking), which is served for detection of 19 road-marking categories in urban scenarios. TRoM was built by means of over one-month data covering a full spectrum of time, weather, and traffic-load. An annotation toolkit was also presented to facilitate enriching such dataset. By directly applying our state-of-the-art method called RPP (ResNet with Pyramid Pooling), a reasonably accurate baseline on TRoM benchmark is made public for further performance comparison and evaluation.},
  pdf={https://drive.google.com/file/d/1uS6tWRAXRuYw3KC9agKElE_JvMTvDFs2/view},
  code={https://github.com/xllau/TRoM_annotation_v1.0}
}

@INPROCEEDINGS{9231638,
abbr={CoG},
  author={Lorenzo, Francesco and Asadi, Sahar and Karnsund, Alice and Cao, Lele and Wang, Tianze and Payberah, Amir H.},
  booktitle={2020 IEEE Conference on Games (CoG)}, 
  title={Use All Your Skills, Not Only The Most Popular Ones}, 
  year={2020},
  volume={},
  number={},
  pages={682-685},
  keywords={Games;Training;Time-frequency analysis;Indexes;Frequency measurement;Benchmark testing;Upper bound;Reinforcement Learning;Deep Q-Network;Intrinsic Rewards;Skill-based Rewards;Candy Crush Friends Saga.},
  doi={10.1109/CoG47356.2020.9231638},
  abstract={Reinforcement Learning (RL) has shown promising results across various domains. However, applying it to develop gameplaying agents is challenging due to sparsity of extrinsic rewards, where agents get rewards from the environments only at the end of game levels. Previous works have shown that using intrinsic rewards is an effective way to deal with such cases. Intrinsic rewards allow to incorporate basic skills in agent policies to better generalize over various game levels. In a gameplay, it is common that certain actions (skills) are observed more often than others, which leads to a biased selection of actions. This problem boils down to a normalization issue in formulating the skill-based reward function. In this paper, we propose a novel solution to this problem by taking into account the frequency of all skills in the reward function. We show that our method improves the performance of agents by enabling them to select effective skills up to 2.5 times more frequently than that of the state-of-the-art in the context of the match-3 game Candy Crush Friends Saga.},
  pdf={https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xM2shP8AAAAJ&citation_for_view=xM2shP8AAAAJ:mVmsd5A6BfQC},
  dimensions={true},
  google_scholar_id={mVmsd5A6BfQC}
}

@inproceedings{Yu2022PayST,
abbr={BMVC},
  title={Pay Self-Attention to Audio-Visual Navigation},
  author={Yinfeng Yu* and Lele Cao* and Fuchun Sun and Xiaohong Liu and Liejun Wang},
  booktitle={Proceedings of British Machine Vision Conference},
  year={2022},
  website={https://yyf17.github.io/FSAAVN/index.html},
  abstract={},
  pdf={https://bmvc2022.mpi-inf.mpg.de/0046.pdf},
  poster={https://bmvc2022.mpi-inf.mpg.de/0046_poster.pdf},
  abstract={Audio-visual embodied navigation, as a hot research topic, aims training a robot to reach an audio target using egocentric visual (from the sensors mounted on the robot) and audio (emitted from the target) input. The audio-visual information fusion strategy is naturally important to the navigation performance, but the state-of-the-art methods still simply concatenate the visual and audio features, potentially ignoring the direct impact of context. Moreover, the existing approaches requires either phase-wise training or additional aid (e.g. topology graph and sound semantics). Up till this date, the work that deals with the more challenging setup with moving target(s) is still rare. As a result, we propose an end-to-end framework FSAAVN (feature self-attention audio-visual navigation) to learn chasing after a moving audio target using a context-aware audio-visual fusion strategy implemented as a self-attention module. Our thorough experiments validate the superior performance (both quantitatively and qualitatively) of FSAAVN in comparison with the state-of-the-arts, and also provide unique insights about the choice of visual modalities, visual/audio encoder backbones and fusion patterns.},
  google_scholar_id={mB3voiENLucC},
  code={https://github.com/yyf17/FSAAVN/tree/main}
}

@inproceedings{10.5555/3060832.3060844,
abbr={IJCAI},
author = {Huang, Wenbing and Cao, Lele and Sun, Fuchun and Zhao, Deli and Liu, Huaping and Yu, Shanshan},
title = {Learning stable linear dynamical systems with the weighted least square method},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Standard subspace algorithms learn Linear Dynamical Systems (LDSs) from time series with the least-square method, where the stability of the system is not naturally guaranteed. In this paper, we propose a novel approach for learning stable systems by enforcing stability directly on the least-square solutions. To this end, we first explore the spectral-radius property of the least-square transition matrix and then determine the key component that incurs the instability of the transition matrix. By multiplying the unstable component with a weight matrix on the right side, we obtain a weighted-least-square transition matrix that is further optimized to minimize the reconstruction error of the state sequence while still maintaining the stable constraint. Comparative experimental evaluations demonstrate that our proposed methods outperform the state-of-the-art methods regarding the reconstruction accuracy and the learning efficiency.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1599–1605},
numpages = {7},
location = {New York, USA},
series = {IJCAI'16},
pdf={https://www.ijcai.org/Proceedings/16/Papers/229.pdf},
website={https://www.ijcai.org/Abstract/16/229},
code={https://github.com/huangwb/LDS-toolbox},
google_scholar_id={8k81kl-MbHgC},
}

@INPROCEEDINGS{8852009,
abbr={IJCNN},
  author={Cao, Lele},
  booktitle={2019 International Joint Conference on Neural Networks}, 
  title={Fine-Grained Road Mining from Satellite Images with Bilateral Xception and DeepLab}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={Roads;Task analysis;Image segmentation;Feature extraction;Satellites;Data mining;Geographic information systems;road extraction;satellite images;convolutional neural network;fusion strategy;classification;segmentation},
  doi={10.1109/IJCNN.2019.8852009},
  dimensions={true},
  google_scholar_id={aqlVkmm33-oC},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/336162565_Fine-Grained_Road_Mining_from_Satellite_Images_with_Bilateral_Xception_and_DeepLab/links/5f5182df92851c250b8ecb7b/Fine-Grained-Road-Mining-from-Satellite-Images-with-Bilateral-Xception-and-DeepLab.pdf},
  abstract={With the recent development of remote sensing and deep learning techniques, automatic and robust road extraction from satellite imaging data has become one of the most popular topics in both fields of Geographic Information System (GIS) and Computer Vision. Despite of the superior performance of Convolutional Neural Networks (DCNNs), a common problem of choosing between the classification and segmentation DCNNs still remains. By comparing two state-of-the-art baseline classification/segmentation DCNNs in several industrial application scenarios, we illustrate that their relative performance may vary, leading to different choices. Based on that observation, we propose a general fusion strategy that conveniently combines the strength of both classification and segmentation DCNNs using an end-to-end network architecture; this paradigm only requires pre-train segmentation/classification DCNNs once, which then can be reused in different road feature mining tasks. The task-specific experiments show that our fusion strategy guarantees superior results in all tested industrial scenarios.},
  code={https://github.com/caolele/road-discovery},
  website={https://patents.google.com/patent/CN109583282B/en}
}

@INPROCEEDINGS{7780796,
abbr={CVPR},
  author={Huang, Wenbing and Sun, Fuchun and Cao, Lele and Zhao, Deli and Liu, Huaping and Harandi, Mehrtash},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Sparse Coding and Dictionary Learning with Linear Dynamical Systems}, 
  year={2016},
  volume={},
  number={},
  pages={3938-3947},
  keywords={Observability;Dictionaries;Encoding;Symmetric matrices;Sparse matrices;Measurement;Computational modeling},
  doi={10.1109/CVPR.2016.427},
  abstract={Linear Dynamical Systems (LDSs) are the fundamental tools for encoding spatio-temporal data in various disciplines. To enhance the performance of LDSs, in this paper, we address the challenging issue of performing sparse coding on the space of LDSs, where both data and dictionary atoms are LDSs. Rather than approximate the extended observability with a finite-order matrix, we represent the space of LDSs by an infinite Grassmannian consisting of the orthonormalized extended observability subspaces. Via a homeomorphic mapping, such Grassmannian is embedded into the space of symmetric matrices, where a tractable objective function can be derived for sparse coding. Then, we propose an efficient method to learn the system parameters of the dictionary atoms explicitly, by imposing the symmetric constraint to the transition matrices of the data and dictionary systems. Moreover, we combine the state covariance into the algorithm formulation, thus further promoting the performance of the models with symmetric transition matrices. Comparative experimental evaluations reveal the superior performance of proposed methods on various tasks including video classification and tactile recognition.},
  pdf={https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Sparse_Coding_and_CVPR_2016_paper.pdf},
  website={https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Sparse_Coding_and_CVPR_2016_paper.html},
  video={https://www.youtube.com/watch?v=0PA1VYyehoQ},
  supp={https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Huang_Sparse_Coding_and_2016_CVPR_supplemental.pdf},
  dimensions={true},
  google_scholar_id={0EnyYjriUFMC}
}

@article{Cao2017,
abbr={Journal},
  author       = {Lele Cao and Fuchun Sun and Hongbo Li and Wenbing Huang},
  title        = {Advancing the incremental fusion of robotic sensory features using online multi-kernel extreme learning machine},
  journal      = {Frontiers of Computer Science},
  year         = {2017},
  volume       = {11},
  number       = {2},
  pages        = {276--289},
  doi          = {10.1007/s11704-016-5171-9},
  url          = {https://doi.org/10.1007/s11704-016-5171-9},
  issn         = {2095-2236},
  dimensions={true},
  google_scholar_id={ULOm3_A8WrAC},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/304405079_Advancing_the_incremental_fusion_of_robotic_sensory_features_using_online_multi-kernel_extreme_learning_machine/links/5c54e7a5a6fdccd6b5db841d/Advancing-the-incremental-fusion-of-robotic-sensory-features-using-online-multi-kernel-extreme-learning-machine.pdf},
  abstract     = {Robot recognition tasks usually require multiple homogeneous or heterogeneous sensors which intrinsically generate sequential, redundant, and storage demanding data with various noise pollution. Thus, online machine learning algorithms performing efficient sensory feature fusion have become a hot topic in robot recognition domain. This paper proposes an online multi-kernel extreme learning machine (OM-ELM) which assembles multiple ELM classifiers and optimizes the kernel weights with a p-norm formulation of multi-kernel learning (MKL) problem. It can be applied in feature fusion applications that require incremental learning over multiple sequential sensory readings. The performance of OM-ELM is tested towards four different robot recognition tasks. By comparing to several state-of-the-art online models for multi-kernel learning, we claim that our method achieves a superior or equivalent training accuracy and generalization ability with less training time. Practical suggestions are also given to aid effective online fusion of robot sensory features.}
}


@INPROCEEDINGS{6977325,
abbr={ICPR},
  author={Cao, Le-le and Huang, Wen-bing and Sun, Fu-chun},
  booktitle={2014 22nd International Conference on Pattern Recognition}, 
  title={Optimization-Based Extreme Learning Machine with Multi-kernel Learning Approach for Classification}, 
  year={2014},
  volume={},
  number={},
  pages={3564-3569},
  keywords={Kernel;Support vector machines;Training;Testing;Optimization;Standards;Mathematical model;multi-kernel extreme learning machine (MK-ELM);extreme learning machine (ELM);multi-kernel learning (MKL);optimization-based ELM;SimpleMKL},
  doi={10.1109/ICPR.2014.613},
  abstract={The optimization method based extreme learning machine (optimization-based ELM) is generalized from single-hidden-layer feed-forward neural networks (SLFNs) by making use of kernels instead of neuron-alike hidden nodes. This approach is known for its high scalability, low computational complexity, and mild optimization constrains. The multi-kernel learning (MKL) framework Simple MKL iteratively determines the combination of kernels by gradient descent wrapping a standard support vector machine (SVM) solver. Simple MKL can be applied to many kinds of supervised learning problems to receive a more stable performance with rapid convergence speed. This paper proposes a new approach: MK-ELM (multi-kernel extreme learning machine) that applies Simple MKL framework to the optimization-based ELM algorithm. The performance analysis on binary classification problems with various scales shows that MK-ELM tends to achieve the best generalization performance as well as being the most insensitive to parameters comparing to optimization-based ELM and Simple MKL. As a result, MK-ELM can be implemented in real applications easily.},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/285625604_Optimization-Based_Extreme_Learning_Machine_with_Multi-kernel_Learning_Approach_for_Classification/links/5c54e896458515a4c751496a/Optimization-Based-Extreme-Learning-Machine-with-Multi-kernel-Learning-Approach-for-Classification.pdf},
  dimensions={true},
  google_scholar_id={zYLM7Y9cAGgC}
}

@article {10.34196/ijm.00120,
abbr={Journal},
article_type = {journal},
title = {The quantitative and qualitative evaluation of a multi-agent microsimulation model for subway carriage design},
author = {Cao, Le-le and Li, Xiao-xue and Kang, Fen-ni and Liu, Chang and Sun, Fu-chun and Kotagiri, Ramamohanarao},
volume = 8,
number = 3,
year = 2015,
month = {dec},
pub_date = {2015-12-31},
pages = {6-40},
citation = {IJM 2015;8(3):6-40},
doi = {10.34196/ijm.00120},
url = {https://doi.org/10.34196/ijm.00120},
abstract = {Multi-agent microsimulation, as a third way of doing science other than induction and deduction methods, is explored to aid subway carriage design in this paper. Realizing that passenger behavior shapes the environment and in turn is shaped by the environment itself, we intend to model this interaction and examine the effectiveness and usability of the proposed model. We address our micro-model from essential aspects of environment space, agent attributes, agent behaviors, simulation process, and global objective/convergence function. Based on the real and simulated data, we evaluate our model with a combination of quantitative and qualitative procedures. For quantitative approach, we proposed two evaluation paradigms (i.e. “unified multinomial classifier” and “one-vs.-all binary classifiers”) using the state-of-the-art machine learning techniques and frameworks; and we manage to show from various perspectives that our model matches the reality in the majority of cases. For qualitative verification, we present a small-scale case study to evaluate different seat layouts in a subway carriage, and identify their advantages and disadvantages with little effort. By enriching microsimulation theory with innovative techniques, our research aims at promoting its acceptance level in design communities by means of avoiding costly creation of real-world experiments.},
keywords = {microsimulation, multi-agent, machine learning, neural network, design, subway carriage, pedestrian flow, qualitative study, quantitative evaluation},
journal = {International Journal of Microsimulation (IJM)},
issn = {1747-5864},
google_scholar_id={ufrVoPGSRksC},
pdf={https://www.microsimulation.pub/download/aHR0cDovL3dlYjo4MDgyLzAwMTIwL2lqbS0wMDEyMC5wZGY=/ijm-00120.pdf?_hash=p7Wo2G%2Fx3krNBgwOuO4rgXNTJfYAEtUZj8zWAXCRNco%3D},
publisher = {International Journal of Microsimulation},
}

@InProceedings{10.1007/978-3-319-31750-2_21,
abbr={PAKDD},
author="Yang, Haolin
and Zhao, Deli
and Cao, Lele
and Sun, Fuchun",
editor="Bailey, James
and Khan, Latifur
and Washio, Takashi
and Dobbie, Gill
and Huang, Joshua Zhexue
and Wang, Ruili",
title="A Precise and Robust Clustering Approach Using Homophilic Degrees of Graph Kernel",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="257--270",
abstract={To address the difficulties of "data noise sensitivity" and "cluster center variance" in mainstream clustering algorithms, we propose a novel robust approach for identifying cluster centers unambiguously from data contaminated with noise; it incorporates the strength of homophilic degrees and graph kernel. Exploiting that in-degrees can breed the homophilic distribution if ordered by their associated sorted out-degrees, it is easy to separate clusters from noise. Then we apply the diffusion kernel to the graph formed by clusters so as to obtain graph kernel matrix, which is treated as the measurement of global similarities. Based on local data densities and global similarities, the proposed approach manages to identify cluster centers precisely. Experiments on various synthetic and real-world databases verify the superiority of our algorithm in comparison with state-of-the-art algorithms.},
isbn="978-3-319-31750-2",
doi="10.1007/978-3-319-31750-2_21",
dimensions={true},
google_scholar_id={kNdYIx-mwKoC},
pdf="https://drive.google.com/file/d/1b2Cc6Y9-D4CZXndh5iTZI6ZplKKMis2x"
}

@INPROCEEDINGS{7727889,
abbr={IJCNN},
  author={Yang, Haolin and Sun, Fuchun and Huang, Wenbing and Cao, Lele and Fang, Bin},
  booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Tactile sequence based object categorization: A Bag of features modeled by Linear Dynamic System with Symmetric Transition Matrix}, 
  year={2016},
  abstract={In this paper, we propose a novel categorization framework to recognize tactile sequences based on two particular properties of the tactile data. For the first one, tactile sequences are spatio-temporal data which is sequential and dynamic, depicting the process of grasping an object in different grasping stages; therefore, it is reasonable to discover the dynamical pattern by modeling tactile data as integral sequences rather than individual frames. For the second one, a tactile sequence contains various dynamical patterns in different stages of the grasping process; therefore, we decompose the whole sequence into multiple mini-sequences so as to enhance feature resolution. To address both properties in our framework, we take advantage of a Bag-of-System model using parameters of the Linear Dynamic System (LDS) as feature descriptors. Moreover, we employ the LDS with Symmetric Transition matrix (LDSST) rather than the original LDS as the building-block in order to obtain accurate codewords of the codebook of the Bag-of-System. The performance of our framework is evaluated on six real-world databases of three groups. Our experiments show that classification using LDSST is better than the original LDS, and the decomposition of tactile sequences does improve the accuracy of classification. The experiment results also show the superiority of our framework in comparison with other state-of-the-art sequence classifiers.},
  volume={},
  number={},
  pages={5218-5225},
  keywords={Grasping;Databases;Symmetric matrices;Feature extraction;Robot sensing systems},
  doi={10.1109/IJCNN.2016.7727889},
dimensions={true},
google_scholar_id={KlAtU1dfN6UC},
pdf={https://drive.google.com/file/d/17Z6CqkqoL_3DATXJmd-DleAiKHyewva-}
}

@INPROCEEDINGS{7288940,
abbr={Conference},
  author={Yang, Haolin and Hu, Xiaohui and Cao, Lele and Sun, Fuchun},
  booktitle={2015 International Conference on Information Science and Technology (ICIST)}, 
  title={A new slip-detection method based on pairwise high frequency components of capacitive sensor signals}, 
  year={2015},
  volume={},
  number={},
  pages={56-61},
  keywords={Discrete wavelet transforms;Market research;Wavelet analysis},
  doi={10.1109/ICIST.2015.7288940},
  abstract={In this paper, a novel method for slip detection using a capacitive sensor is proposed. We perform the Discrete Wavelet Transform (DWT) on the original signals of sensor. By comparing different wavelets, we find that the Haar wavelet is the most suitable to separate different frequency components. After performing the DWT by using the Haar wavelet, the separated high frequency components are pairwise due to properties of the Haar wavelet. Different from setting thresholds to detect object slip, our method detects slip by observing the variation trend of pairwise high frequency components. Meanwhile, we can distinguish signals of object loading and slip respectively. We carry out experiments on several objects with different surface properties and the results are consistent with our observations.},
  pdf={https://www.researchgate.net/profile/Lele-Cao/publication/308847708_A_new_slip-detection_method_based_on_pairwise_high_frequency_components_of_capacitive_sensor_signals/links/5b20e1ff458515270fc610f5/A-new-slip-detection-method-based-on-pairwise-high-frequency-components-of-capacitive-sensor-signals.pdf},
  google_scholar_id={5nxA0vEk-isC},
  dimensions={true}
}

@INPROCEEDINGS{6997629,
abbr={Conference},
  author={Cao, Le-le and Huang, Wen-bing and Sun, Fu-chun},
  booktitle={2014 International Conference on Multisensor Fusion and Information Integration for Intelligent Systems (MFI)}, 
  title={Optimization-based multikernel extreme learning for multimodal object image classification}, 
  year={2014},
  volume={},
  number={},
  pages={1-9},
  keywords={Kernel;Visualization;Support vector machines;Training;Standards;Optimization methods;Histograms},
  doi={10.1109/MFI.2014.6997629},
  abstract={This paper is concerned with multi-kernel extreme learning machine (MK-ELM) which adapts the multi-kernel learning (MKL) framework to extreme learning machine (ELM). MK-ELM approach iteratively determines the combination of kernels by gradient descent wrapping a standard optimization method based ELM. Such MKL methods are very useful in information fusion research and applications. MK-ELM's performance on object image classification via multimodal feature (visual and textual) fusion is experimented and studied. By comparing to other widely used fusion methods (i.e. SVM-based SimpleMKL, feature concatenation, and decision fusion), several advantages and characteristics of MK-ELM fusion are revealed and discussed showing MK-ELM is an easy and effective approach to implement in object image classification applications.},
  pdf={https://drive.google.com/file/u/0/d/1XFDzRVk2UbMc_VDlwAaE0b-5XWrFt3K-},
  google_scholar_id={Tyk-4Ss8FVUC}
}












